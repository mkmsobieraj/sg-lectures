---
title: "Wykład II - Regresja logistyczna w R"
author: "Mateusz Sobieraj"
date: "21 marca 2017"
encoding: "UTF-8"
output: 
  ioslides_presentation:
    css: dependencies/styles.css
    widescreen: false
    transition : "slower"
    smaller: true
    logo: dependencies/grosz.png

---

<!-- 'f' enable fullscreen mode -->
<!-- 'w' toggle widescreen mode -->
<!-- 'o' enable overview mode -->
<!-- 'h' enable code highlight mode -->
<!-- 'p' show presenter notes -->

## Plan wykładu {.build}
  
- Teoria
    - Oznaczenia
    - Przypomnienie
    - Dlaczego funkcja logit?
    - Alternatywy do funkcji logit
    - Bonus
- R
    - Rzut oka na dane
    - Przygotowanie danych
    - Regresja logistyczna
    - Tabela kontyngencji
    - Optymalizacja punktu odcięcia
    
## Teoria {.build}
  
## Oznaczenia {.build}

W dalszej części wykłady będziemy przyjmować następujące oznaczenia:

- $Y$ - zmienna objaśniana
- $p_i$ - prawdopodobieństwo sukcesu dla i-tej próby (szukana wartość)
- $X$ - wektor zmiennych objaśniających
- $E$ - wartość oczekiwana 
- $Odds$ - Szansa $\frac{p}{1 - p}$
- $X$, $Y$ itp. - zmienne losowe
- $x_i$, $y_i$ - realizacje zmiennych losowych

## Przypomnienie {.build}

##### Założenie

Regresja logistyczna zakłada, że $Y$ ma warunkowy rozkład Beornuliego:
  $$
  Y| X = x_i \sim B(p_i, 1)
  $$

#### Wniosek 

1. $E(Y|X = x_i) = p_i$ (dzięki temu regresja logistyczna działa!)
2. $P(Y = y | X =x_i) = p_i^y(1 - p_i)^{(1-y)}$

#### Dowód 

1. $E(Y|X = x_i) = P(Y = 1 |X = x_i) * 1 + P(Y = 0|X = x_i) * 0 =$
   $= P(Y = 1|X = x_i) = p_i$
2. $P(Y = 1 | X = x_i) = p_i^1 (1 - p_i)^{(1 - 1)} = p_i$
   $P(Y = 0 | X = x_i) = p_i^0 (1 - p_i)^{(1 - 0)} = 1 - p_i$

## Dlaczego funkcja logit? {.build}

Chcemy stworzyć model, który powie nam czy pod warunkiem $X = x_i$ realizacja zmiennej $Y$ zakończy się sukcesem (1) czy porażką (0). Nie wiemy jak to zrobić, ale jesteśmy całkiem nieźli w modelach liniowych ;) .

Tworzymy zatem model liniowy 

$$ E(Y|X) = \beta X $$
Jest prawie dobrze, ale $Y$ jako kombinacja liniowa zmiennych, które przeważnie nie są ograniczone, może przyjąć dowolnie duże (małe) wartości, w szczególności różne od $\{0, 1\}$ .  

## Dlaczego funkcja logit? II {.build}

Nic straconego!!! W liceum poznaliśmy funkcje monotoniczne, użyjmy ich! 

$$
f(x) = \{
 \begin{array}{cc}
1 & jeżeli \ x > 0  \\
0 & jeżeli \ x < 0
\end{array}
$$
```{r required_packages, include=FALSE, eval=TRUE}
#install.packages("GGally")
# install.packages("SDMTools")
#install.packages("leaps")
require("leaps")
require('ggplot2')
require("GGally")
require("dplyr")
require("caret")
require("SDMTools")
setwd("D:/GIT/sg-lectures")
fly<-read.csv("dependencies/FlightDelays.csv")
fly <- fly %>% select(- flightnumber, -tailnu, -date)
```

```{r ex_function, echo=FALSE, fig.height=3.5}
x <- seq(-10, 10, by = 0.1)
f_x <- ifelse(x > 0, 1, 0)
dt1 <- data.frame(x_val = x, y_val = f_x)
ggplot(data = dt1, aes(x_val, y_val)) + geom_point(colour = rgb(1,  0.463, 0.165))
```

## Dlaczego funkcja logit? III {.build}

Udało się! Wystarczy przyłożyć funkcję $f$ do prawej strony równania 
$$ E(Y|X) = \beta X $$

i otrzymujemy 

$$ E(Y|X) = f(\beta X) = \{
 \begin{array}{cc}
1 & jeżeli \ \beta X > 0  \\
0 & jeżeli \ \beta X < 0
\end{array} $$

## Dlaczego funkcja logit? IV {.build}

#### Problemy 

1. Co zrobić gdy $\beta X_i = 0$? (z tym łatwo sobie poradzić zmieniając jedną z nierówności na słabą)
2. Funkcja jest nieciągła i nieróżniczkowalna co sprawia wiele problemów formalnych. Przykładowo, gdy chcemy wyznaczyć współczynniki $\beta$ to co zwykle robimy to maksymalizujemy funkcję wiarygodności 
  $$
  L(\beta) = \prod_{i: \ y_i = 1}p(x_i)\prod_{i: \ y_i = 0}(1 - p(x_i))
  $$
  3. Funkcja jest idealna do problemów liniowo separowanych to co tak naprawdę zrobiliśmy to poprowadziliśmy hiperpłaszczyznę i powiedzieliśmy wszystko z jednej strony jest sukcesem a z drugiej porażką. Jednak w rzeczywistości mało który problem jest liniowo separowany (pomysł nie jest zupełnie chybiony [perceptron](https://en.wikipedia.org/wiki/Perceptron)).
ale jest ona nieróżniczkowalna, więc nie możemy znaleźć analitycznie maksimum!

## Dlaczego funkcja logit? V {.build}

#### Definicja (Logit i funkcja odwrotna do logit)

1. logit - $logit(p) = ln(\frac{p}{1-p}) = ln(odds)$
2. funkcja odwrotna - $p = \frac{1}{1 + \exp(-odds)}$

```{r logit, echo=FALSE}
s <- seq(0, 1, by = 0.001)
logit <- data.frame(x = s, y = log(s / (1 - s)))
ggplot(data = logit, aes(x = x, y = y)) + geom_point(colour = rgb(1,  0.463, 0.165)) +
  ggtitle("logit")
```

## Dlaczego funkcja logit? VI {.build}
```{r r_logit, echo=FALSE}
s_x <- seq(-10, 10, by = 0.1)
s_y <- 1 / (1 + exp(-s_x))
logit <- data.frame(x = s_x, y = s_y)
ggplot(data = logit, aes(x = x, y = y)) + geom_point(colour = rgb(1,  0.463, 0.165)) +
  ggtitle("funkcja odwrotna do logit")
```

## Dlaczego funkcja logit? VII {.build}

Użyjmy funkcji logit do naszego problemu.

$$
 logit(E(Y|X)) = \beta X
$$
zatem 
$$
 E(Y|X) = \frac{e^{\beta X}}{1 + e^{\beta X}}
$$

## Dlaczego funkcja logit? VIII {.build}

#### Brak problemów 
1. $\beta X_i = 0$ wówczas $\frac{e^{0.5}}{1 + e^{0.5}} = 0.5$
2. funkcja jest ciągła monotoniczna i jest klasy $C^\infty$
3. Poprowadziliśmy idealnie taką samą hiperpłaszczyznę co wcześniej, ale teraz, w zależności od tego po której stronie hiperpłaszczyzny znajdzie się obserwacja i w jakiej odległości przypisujemy jej wartość od 0 do 1 co naturalnie można utożsamiać z prawdopodobieństwem sukcesu (bo $E(Y| X = x) =p$).

<div class="notes">

Jest to w 100% intuicyjne, naturalnym wydaje się, że im dalej obserwacja znajduje się od hiperpłaszczyzny tym mniejsze prawdopodobieństwo pomyłki.

</div>

## Alternatywy do funkcji logit {.build}
```{r r_alt, echo=FALSE, fig.height=3}
x <- seq(-10, 10, by = 0.1)
probit <- pnorm(x)
logit <- 1 / (1 + exp(-x))
th <- tanh(x) / 2 + 0.5
t_student <- pt(x, df = 1)

dt <- data.frame(x_val = rep(x, 4), y_val = c(probit, logit, th, t_student), label = c(rep("probit", 201), rep("logit", 201), rep("tanh / 2 + 0.5", 201), rep("t-studenta", 201)))

ggplot(dt, aes(x_val, y_val, colour = label)) + geom_line()

```

#### Uwaga
Tak naprawdę mogę wziąć dowolną dystrybuantę rozkładu ciągłego określonego na całej osi $\mathbb{R}$ (Niektóre to zły pomysł np. dystrybuanta rozkładu wykładniczego jest nieróżniczkowalna w 0).

## Bonus I{.build}
Dlaczego nie należy rozważać wszystkich możliwych modeli?

#### Scenariusz 
Załóżmy, że mamy 100 zmiennych i chcemy by do modelu weszło 5 z nich. By wybrać najlepsze tworzymy wszystkie możliwe modele i wybieramy najlepszy z modeli za pomocą jakiegoś kryterium informacyjnego np [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion).

#### Problemy 
1. Może być to uciążliwe obliczeniowo w powyższym scenariuszu musimy stworzyć.

$${100 \choose 5} = \frac{100!}{95!*5!} = 75 \ 287 \ 520$$
2. Tworząc bardzo dużą liczbę modeli jesteśmy narażeni na 'overfitting'. 

## Bonus I cd{.build}

Jeżeli bardzo chcemy, rozważyć wszystkie możliwe modele możemy użyć funkcji `regsubsets` z pakietu `leaps`

- `nvmax` - maksymalna dozwolona liczba zmiennych
- w wierszach mamy wielkość podzbioru (z ilu zmiennych się składa) (w naszym przypadku 1 - 5)
- \* - oznacza czy zmienna znajduje się w tym podzbiorze 
- podzbiory nie muszą być w sobie zawarte (znajduje on najlepsze podzbiory)

## Bonus I cd{.build}

```{r leaps}
regfit_full <- regsubsets(Sepal.Length ~ ., data = iris, nvmax = 5)
summary(regfit_full)
```

## Bonus II {.build}

Jaka jest interpretacja zmiany wartości jednej zmiennej objaśniającej?

W modelu linowym jest prosto np. mamy model
  $$ y = -7 + 5x_1 + 3x_2 - 0.5x_3$$

zmiana $x_2$ o jeden zmienia $y$ o 3 (niezależnie od wartości pozostałych zmiennych). W regresji logistycznej jest to trochę ciężej interpretowalne. Przypomnijmy, że z wcześniejszych rozważań dostaliśmy 
$$
 p = E(Y|X) = \frac{e^{\beta X}}{1 + e^{\beta X}} = \frac{1}{1 + e^{-\beta X}}
$$
zatem dla realizacji zmiennej losowej $X = (1, 3, 10)$ i powyższego modelu mamy

## Bonus III {.build}

$$p_i = \frac{1}{1 + e^{-(-7 + 5x_1 + 3x_2 - 0.5x_3)}}$$
$$p_i = \frac{1}{1 + e^{-(-7 + 5*1 + 3*2 - 0.5*10)}} = 0.731$$
załóżmy, że chcemy zobaczyć co powoduje zmiana zmiennej $X_3$ o jeden:
$$p_i = \frac{1}{1 + e^{-(-7 + 5*1 + 3*2 - 0.5*9)}} = 0.623$$
$$p_i = \frac{1}{1 + e^{-(-7 + 5*1 + 3*2 - 0.5*11)}} = 0.818$$

## Bonus IV {.build}

Nie jest to zmiana liniowa $|0.731 - 0.623| = 0.108$ oraz $|0.731 - 0.818| = 0.109$ bo funkcja $\frac{1}{1 + e^{-\beta X}}$ nie jest liniowa. Co więcej istnieje zależność od wartości pozostałych zmiennych (dlatego ustaliliśmy wartości pozostałych zmiennych poza $X_3$) i od kierunku w którym się poruszamy. 

```{r bonus, echo=FALSE}
x <- seq(-5, 20, by = 0.1)
logit <- 1 / (1 + exp(-7 + 5*1 + 3*2 - 0.5*x))
extra_points <- data.frame(x_val = c(9, 10, 11), y_val = c(0.623, 0.731, 0.818), label = c("new", "old", "new"))

dt <- data.frame(x_val = x, y_val = logit)

ggplot(dt, aes(x_val, y_val)) + geom_line() +
  geom_point(data = extra_points, aes(x_val, y_val, colour = label), cex = 4)

```

## Bonus V {.build}

Co zrobić, gdy w próbie mamy $\hat{p} = 30\%$ sukcesów a w populacji $p = 10\%$ ? 

Wystarczy skorygować parametr $\hat{\beta_0}$ w następujący sposób

$$\beta = \hat{\beta_0} + \ln\frac{p}{1 - p} - \ln\frac{\hat{p}}{1 - \hat{p}}$$

#### Dowód

Weźmy otrzymany model 
$$ \ln\frac{\hat{p}}{1- \hat{p}} = \hat{\beta_0} + \beta_1 X_1 + \ldots +\beta_n X_n$$ 
i płóżmy $\beta = \hat{\beta_0} + \ln\frac{p}{1 - p} - \ln\frac{\hat{p}}{1 - \hat{p}}$ zamiast $\beta_0$. WóWczas
$$ \ln\frac{\hat{p}}{1- \hat{p}} = \beta_0 - \ln\frac{p}{1 - p} + \ln\frac{\hat{p}}{1 - \hat{p}} + \beta_1 X_1 + \ldots +\beta_n X_n$$ 
$$ \ln\frac{\hat{p}}{1- \hat{p}} + \ln\frac{p}{1 - p} - \ln\frac{\hat{p}}{1 - \hat{p}} = \beta_0 + \beta_1 X_1 + \ldots +\beta_n X_n$$
$$ \ln\frac{p}{1 - p}= \beta_0 + \beta_1 X_1 + \ldots +\beta_n X_n$$


## R {.build}

## Rzut oka na dane I {.build}

Użyjemy danych o opóźnieniach lotów. Zbiór danych zawiera 2201
lotów, które się odbyły w styczniu 2004 z dystryktu Washington DC do dystryktu NYC.
Flaga `delayed`  jest nadana, gdy opóźnienie wynosi więcej niż 15 minut (0 dla nieopóźnionych i 1 dla opóźnionych).

nr col|col name|description
---|---|---
[, 1]|schedtime|
[, 2]|carrier|
[, 3]|deptime|
[, 4]|dest|arrival airports (Kennedy, Newark, and LaGuardia)

## Rzut oka na dane I cd {.build}

nr col|col name|description
---|---|---
[, 5]|distance|
[, 6]|origin|departure airports (Reagan, Dulles, and Baltimore)
[, 7]|weather|
[, 8]|dayweek|
[,9]|daymonth|
[,10]|delay|

## Rzut oka na dane II {.build}

```{r fly_rev1}
str(fly)
```

## Rzut oka na dane III {.build}

```{r fly_rev2}
summary(fly)
```

## Rzut oka na dane IV {.build}

```{r fly_rev3, results="hide"}
my_dens <- function(data, mapping, ...) {
  ggplot(data = data, mapping=mapping) +
    geom_density(..., alpha = 0.7, color = NA) 
}

pairs_plot <-GGally::ggpairs(fly, columns = c("distance", "schedtime", "dest"), 
                mapping = aes(color  = delay, fill  = delay),
                diag = list(continuous = my_dens)
                )
```

<div class="notes">

schedtime x distance korelacja w całej populacji i podgrupach

</div>

## Rzut oka na dane IV cd {.build}

```{r fly_rev4, echo=FALSE}
pairs_plot
```

## Przygotowanie danych {.build}
```{r train_test}
train <- sample_frac(fly, 0.6) # dplyr
test <- anti_join(fly, train) # dplyr
```

## Regresja logistyczna {.build}

```{r reg_log1, warning=FALSE}
m1 <- glm(delay ~ ., data = train, family = binomial)
summary(m1)
```

## Regresja logistyczna {.build} cd.

```{r reg_log1cd, warning=FALSE}
m2 <- glm(delay ~ schedtime + carrier + deptime, data = train, family = binomial)
summary(m2)
```

## Regresja logistyczna II {.build}

Test który model jest lepszy

Wykonamy [Likelihood-ratio test](https://en.wikipedia.org/wiki/Likelihood-ratio_test), czyli test który sprawdza czy model bardziej złożony jest lepszy (Hipoteza $H_0$ model mniej złożony jest lepszy).

```{r, lrt}
anova(m1, m2, test = "Chisq")
```


## Regresja logistyczna III {.build}

Dokonywanie predykcji na zbiorze testowym 

```{r predict_val}
pred <- predict(m1, test, type = "response")
klasyfikacja <- as.vector(ifelse(pred > 0.5, 0, 1)) # sprubujmy 0.5
actuals <- ifelse(test$delay == 'ontime', 0, 1)
confusionMatrix(table(klasyfikacja, actuals))

```

## Tabela kontyngencji {.build}

[Confusion matrix](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)

[Tabela kontyngencji](https://pl.wikipedia.org/wiki/Swoisto%C5%9B%C4%87_testu_diagnostycznego)

statystyka |wzór|opis 
---|---|---
`Sensitivity`(`Czułość`)|$\frac{TP}{TP + FN}$|zdolność modelu do prawidłowej klasyfikacji zajścia zdarzenia (im wyższa czułość tym niższy błąd I rodzaju).
`Specificity`(`Swoistość`)|$\frac{TN}{TN + FP}$|zdolność modelu do prawidłowej klasyfikacji nie zajścia zdarzenia(im wyższa swoistość tym niższy błąd II rodzaju).
`Precision`(`Precyzja`)|$\frac{TP + TN}{TP + TN + FP + FN}$| mierzy obciążenie modelu ([bias](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff))
`Accuracy`(`Dokładnść`)|$\frac{TP}{TP + FP}$| mierzy wariancje modelu ([variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff))
`Misclassification rate`(`Błąd klasyfikacji`)|$\frac{FP + FN}{TP + Tn + FP + Fn}$| ogólny błąd predykcji

## Optymalizacja punktu odcięcia {.build}

```{r}
# SDMTools
optimal <- optim.thresh(actuals, pred)
optimal
```

## Optymalizacja punktu odcięcia II

Czy udało nam się poprawić wynik? {.build}

```{r}
klasyfikacja <- ifelse(pred > 0.85, 0, 1)
confusionMatrix(table(klasyfikacja, actuals)) # caret
confusionMatrix
```
