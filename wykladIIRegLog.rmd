---
title: "Wykład II - Regresja logistyczna w R"
author: "Mateusz Sobieraj"
date: "13 marca 2017"
encoding: "UTF-8"
output: 
  ioslides_presentation:
  css: dependencies/styles.css
  widescreen: true
  logo: dependencies/grosz.png

---

## Plan wykładu
  
- Teoria
    - oznaczenia
    - przypomnienie
    - dlaczego funkcja logit
    - alternatywy do funkcji logit
    - bonus
- R
    - Rzut oka na dane
    - Przygotowanie danych
## Teoria
  
## Oznaczenia

W dalszej części wykłady będziemy przyjmować następujące oznaczenia:

- $Y_i$ - zmienna objaśniana
- $p_i$ - prawdopodobieństwo sukcesu dla i-tej próby (szukana wartość)
- $X$ - wektor zmiennych objśniających
- $E$ - wartość oczekiwana 
- $Odds$ - Szansa $\frac{p}{1 - p}$
- $X$, $Y$ itp. - zmienne losowe
- $x_i$, $y_i$ - realizacje zmiennych losowych

## Przypomnienie 

##### Założenie

Regresja logistyczna zakłada, że $Y_i$ ma rozkład Beornuliego:
  $$
  Y| X = x_i \sim B(p_i, 1)
  $$

#### Wniosek 

1. $E(Y|X = x_i) = p_i$ (dzięki temu regresja logistyczna działa!)
2. $P(Y = y | X =x_i) = p_i^y(1 - p_i)^{(1-y)}$

#### Dowód 

1. $E(Y|X = x_i) = P(Y = 1 |X = x_i) * 1 + P(Y = 0|X = x_i) * 0 =$
   $= P(Y = 1|X = x_i) = p_i$
2. $P(Y = 1 | X = x_i) = p_i^1 (1 - p_i)^{(1 - 1)} = p_i$
   $P(Y = 0 | X = x_i) = p_i^0 (1 - p_i)^{(1 - 0)} = 1 - p_i$

## Dlaczogo funkcja logit?

Chcemy stworzyć model który powie nam czy pod warunkiem $X = x_i$ realizacja zmiennej $Y$ zakończy się sukcesem (1) czy porażkom (0). Nie wiemy jak to zrobić ale jesteśmy całkiem nieźli w modelach liniowych ;) .

Tworzymy zatem model liniowy 

$$ E(Y|X) = \beta X $$
Jest prawie dobrze ale $Y_i$ jako kombinacja liniowa zmiennych, które przeważnie nie są ograniczone, może przyjąć dowolnie duże (małe) wartości, w szczegulności różne od $\{0, 1\}$ .  

## Dlaczogo funkcja logit? II

Nic straconego!!! W liceum poznaliśmy funkcje monotoniczne, użyjmy ich! 

$$
f(x) = \{
 \begin{array}{cc}
1 & jeżeli \ x > 0  \\
0 & jeżeli \ x < 0
\end{array}
$$
```{r required_packages, include=FALSE, eval=TRUE}
#install.packages("GGally")
require('ggplot2')
require("GGally")
require("dplyr")
require("caret")
setwd("D:/GIT/sg-lectures")
fly<-read.csv("dependencies/FlightDelays.csv")
fly <- fly %>% select(- flightnumber, -tailnu, -date)
```

```{r ex_function, echo=FALSE, fig.height=3.5}
x <- seq(-10, 10, by = 0.1)
f_x <- ifelse(x > 0, 1, 0)
dt1 <- data.frame(x_val = x, y_val = f_x)
ggplot(data = dt1, aes(x_val, y_val)) + geom_point(colour = rgb(1,  0.463, 0.165))
```

## Dlaczogo funkcja logit? III

udało się! wystarczy przyłożyć funkcję $f$ do prawej strony równania 
$$ E(Y) = \beta X $$

i otrzymujemy 

$$ E(Y) = f(\beta X) = \{
 \begin{array}{cc}
1 & jeżeli \ \beta X > 0  \\
0 & jeżeli \ \beta X < 0
\end{array} $$

## Dlaczogo funkcja logit? IV

#### Problemy 

1. co gdy $\beta X_i = 0$ (z tym łątwo sobie poradzić zmieniając jedną z nierówności na słabom)
2. funkcja jest nieciągła i nieróżniczkowalna co sprawia wiele problemów formalnych. Przykłądowo gdy chemy wyznaczyć wspułćzynniki $\beta$ to co robimy to maksymalizujemy funkcję wiraygodności 
  $$
  L(\beta) = \prod_{i: \ y_i = 1}p(x_i)\prod_{i: \ y_i = 0}(1 - p(x_i))
  $$
ale jest ona nierużniczkowalna, więc nie możemy znaleźć analitycznie maksimum!

## Dlaczogo funkcja logit? IV cd
<ol start = 3>
<li>
funkcja jest idelana do problemów liniowo separowalnych to co tak napradę zrobiliśmy to poprowadziliśmy hiperpłaszczyznę i powiedzieliśmy wszystko z jednej strony jest sukcesem a z drugiej poraszką. Jednak w żeczywistości mało który problem jest liniowo separowalny (pomysł nie jest zupełnie chybiony [perceptron](https://en.wikipedia.org/wiki/Perceptron)).
</li>
</ol>
## Dlaczogo funkcja logit? V

#### Definicja (Logit i funkcja odwrotna do logit)

1. logit - $logit(p) = ln(\frac{p}{1-p}) = ln(odds)$
2. funkcja odwrotna - $p = \frac{1}{1 + \exp(-odds)}$


## Dlaczogo funkcja logit? VI
```{r logit, echo=FALSE}
s <- seq(0, 1, by = 0.001)
logit <- data.frame(x = s, y = log(s / (1 - s)))
ggplot(data = logit, aes(x = x, y = y)) + geom_point(colour = rgb(1,  0.463, 0.165)) +
  ggtitle("logit")
```


## Dlaczogo funkcja logit? VII
```{r r_logit, echo=FALSE}
s_x <- seq(-10, 10, by = 0.1)
s_y <- 1 / (1 + exp(-s_x))
logit <- data.frame(x = s_x, y = s_y)
ggplot(data = logit, aes(x = x, y = y)) + geom_point(colour = rgb(1,  0.463, 0.165)) +
  ggtitle("funkcja odwrotna do logit")
```

## Dlaczogo funkcja logit? VIII

Użyjmy funkcj logit do naszego problemu 

$$
 logit(E(Y|X)) = \beta X
$$
zatem 
$$
 E(Y|X) = \frac{e^{\beta X}}{1 + e^{\beta X}}
$$

## Dlaczogo funkcja logit? IX

#### Brak problemów 
1. $\beta X_i = 0$ wóWczas $\frac{e^{0.5}}{1 + e^{0.5}} = 0.5$
2. funkcja jest ciągła monotoniczna i jest klasy $C^\infty$
3. Poprowadziliśmy idealnie taką samą hiperpłąszczyznę co wczejśniej, ale teraz, w zależności od tego po której stronie hiperpłaszczyzny znajdzie się obserwacja i w jakiej odległości przypisujemy jej wartość od 0 do 1 co naturalnie można utożsamiać z prawdopodobieństwem sukcesu (bo $E(Y| X = x) =p$).

## alternatywy do funkcji logit
```{r r_alt, echo=FALSE, fig.height=3}
x <- seq(-10, 10, by = 0.1)
probit <- pnorm(x)
logit <- 1 / (1 + exp(-x))
th <- tanh(x) / 2 + 0.5
t_student <- pt(x, df = 1)

dt <- data.frame(x_val = rep(x, 4), y_val = c(probit, logit, th, t_student), label = c(rep("probit", 201), rep("logit", 201), rep("tanh / 2 + 0.5", 201), rep("t-studenta", 201)))

ggplot(dt, aes(x_val, y_val, colour = label)) + geom_line()

```

#### Uwaga
Tak naprawdę mogę wziąć dowolną dystrybuantę rozkładu ciągłego orkeślonego na całej osi $\mathbb{R}$ (Nietkóre to zły pomysł np. dystrybuanta rozkładu wykładniczego jest nieróżniczkowalna w 0).

## Bonus I 
Dlaczego nie należy rozważać wszystkich możliwych modeli?

#### Scenariusz 
Załóżmy, że mamy 100 zmiennych i chemy by do modelu weszlo 5 z nich, by wybrać najlepsze tworzymy wszystkie możliwe modele i wybieramy najlepszy z modeli za pomocą jakiegoś kryterium informacyjnego  
np [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion).

## Bonus II

#### Problemy 
1. Może być to uciążliwe obliczeniowo w powyższym scenariuszu musimy stowrzyć

$${100 \choose 5} = \frac{100!}{95!*5!} = 75 \ 287 \ 520$$
2. Tworząc bardzo dużą liczbę modeli jesteśmy narażeni na 'overfitting'. 

## Bonus III
Jaka jest interpretacja zmianny wartości jednej zmiennej objaśniającej?

W modelu linowym jest prosto np. mamy model
  $$ y = -7 + 5x_1 + 3x_2 - 0.5x_3$$
zmiana $x_2$ o jeden zmienia $y$ o 3. W regresji logistycznej jest to trochę ciężej interpretowalne. Przypomnijmy, że z wcześniejszych rozważań dostaliśmy 
$$
 p = E(Y|X) = \frac{e^{\beta X}}{1 + e^{\beta X}} = \frac{1}{1 + e^{-\beta X}}
$$
zatem dla ralizacji zmiennej losowej $X = (1, 3, 10)$ i powyższego modelu mamy

## Bonus IV

$$p_i = \frac{1}{1 + e^{-(-7 + 5x_1 + 3x_2 - 0.5x_3)}}$$
$$p_i = \frac{1}{1 + e^{-(-7 + 5*1 + 3*2 - 0.5*10)}} = 0.731$$
załóżmy, że chcemy zobaczyć co powoduje zmiana zmienej $X_3$ o jeden:
$$p_i = \frac{1}{1 + e^{-(-7 + 5*1 + 3*2 - 0.5*9)}} = 0.623$$
$$p_i = \frac{1}{1 + e^{-(-7 + 5*1 + 3*2 - 0.5*11)}} = 0.818$$

## Bonus V

Nie jest to zmiana liniowa $|0.731 - 0.623| = 0.108$ oraz $|0.731 - 0.818| = 0.109$ bo funkcja $\frac{1}{1 + e^{-\beta X}}$ nie jest liniowa, zatem zmiana zależy od wartości pozostałych zmiennych (dlatego ustaliliśmy wartości pozostałych zmiennych poza $X_3$) i od kierunku w którym się poruszamy. 

## Bonus VI

```{r bonus, echo=FALSE}
x <- seq(-5, 20, by = 0.1)
logit <- 1 / (1 + exp(-7 + 5*1 + 3*2 - 0.5*x))
extra_points <- data.frame(x_val = c(9, 10, 11), y_val = c(0.623, 0.731, 0.818), label = c("new", "old", "new"))

dt <- data.frame(x_val = x, y_val = logit)

ggplot(dt, aes(x_val, y_val)) + geom_line() +
  geom_point(data = extra_points, aes(x_val, y_val, colour = label), cex = 4)

```

## Bonus VII

Co zrobić w próbie mamy $\hat{p} = 30\%$ sukcesów a w populacji $p = 10\%$ ? 

Wystarczy skorygować parametr $\hat{\beta_0}$ w stępujący sposób

$$\beta = \hat{\beta_0} + \ln\frac{p}{1 - p} - \ln\frac{\hat{p}}{1 - \hat{p}}$$

## R

## Rzut oka na dane I

We use data set on delayed airplanes. The data set consists of 2201
airplane flights in January 2004 from the Washington DC area into the NYC area.
The characteristic of interest (the response) is whether or not a flight has been
delayed by more than 15 min (coded as 0 for no delay, and 1 for delay).

nr col|col name|description
---|---|---
[, 1]|schedtime|
[, 2]|carrier|
[, 3]|deptime|
[, 4]|dest| departure airports (Reagan, Dulles, and Baltimore)

## Rzut oka na dane I cd

nr col|col name|description
---|---|---
[, 5]|distance|
[, 6]|origin|arrival airports (Kennedy, Newark, and LaGuardia)
[, 7]|weather|
[, 8]|vs|
[, 9]|dayweek|
[,10]|daymonth|
[,11]|delay|

## Rzut oka na dane II

```{r fly_rev1}
str(fly)
```

## Rzut oka na dane III

```{r fly_rev2}
summary(fly)
```

## Rzut oka na dane IV

```{r fly_rev3, results="hide"}
my_dens <- function(data, mapping, ...) {
  ggplot(data = data, mapping=mapping) +
    geom_density(..., alpha = 0.7, color = NA) 
}

pairs_plot <-GGally::ggpairs(fly, columns = c("distance", "schedtime", "dest"), 
                mapping = aes(color  = delay, fill  = delay),
                diag = list(continuous = my_dens)
                )
```

## Rzut oka na dane IV cd

```{r fly_rev4, echo=FALSE}
pairs_plot
```

## przygotowanie danych
```{r train_test}
train <- sample_frac(fly, 0.6) # dplyr
test <- anti_join(fly, train) # dplyr
```

## Regresja logistyczna 

```{r reg_log1}
m1 <- glm(delay ~ ., data = train, family = binomial)
summary(m1)

m2 <- glm(delay ~ schedtime + carrier + deptime, data = train, family = binomial)
summary(m2)
```

## Test któy model jest lepszy 

Wykonamy [Likelihood-ratio test](https://en.wikipedia.org/wiki/Likelihood-ratio_test), czyli test któy sprawdza czy model bardziej złożony jest lepszy (Hipoteza $H_0$ model mniej złożony jest lepszy.)

```{r, lrt}
anova(m1, m2, test = "Chisq")
```


## Dokonywanie predykcj na zboirze testowym 

```{r predict_val}
pred <- predict(m1, test, type = "response")
klasyfikacja <- as.vector(ifelse(pred > 0.5, 0, 1)) # sprubujmy 0.5
actuals <- ifelse(test$delay == 'ontime', 0, 1)
confusionMatrix(table(klasyfikacja, actuals))

```

## Tabela kontyngencji

[Confusion matrix](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)

[Tabela kontyngencji](https://pl.wikipedia.org/wiki/Swoisto%C5%9B%C4%87_testu_diagnostycznego)

## optymalizacja punktu odcięcia 

```{r}
# install.packages("SDMTools")
require("SDMTools")

optimal <- optim.thresh(actuals, pred)

optimal
```

## Czy udało nam się poprawić wynik?

```{r}
klasyfikacja <- ifelse(pred > 0.87, 0, 1)

confusionMatrix(table(klasyfikacja, actuals))

```
