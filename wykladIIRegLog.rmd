---
title: "Wykład II - Regresja logistyczna w R"
author: "Mateusz Sobieraj"
date: "13 marca 2017"
encoding: "UTF-8"
output: 
  ioslides_presentation:
  css: dependencies/styles.css
logo: dependencies/grosz.png

---

## Plan wykładu
  
- przypomnienie teorii
- regresja logistyczna w R
  
## Oznaczenia

W dalszej części wykłady będziemy przyjmować następujące oznaczenia:

- $Y_i$ - zmienna objaśniana dla i-tej próby (przyjmuje tylko wartości 0 i 1)
- $p_i$ - prawdopodobieństwo sukcesu dla i-tej próby (szukana wartość)
- $X$ - wektor zmiennych objśniających
- $E$ - wartość oczekiwana 
- $Odds$ - Szansa $\frac{p}{1 - p}$

## Przypomnienie 

##### Założenie

Regresja logistyczna zakłada, że $Y_i$ ma rozkład Beornuliego:
  $$
  Y_i| X = x \sim B(p_i, 1)
  $$

#### Wniosek 

1. $E(Y_i|X = x) = p_i$ (dzięki temu regresja logistyczna działa!)
2. $P(Y_i = y | X =x) = p_i^y(1 - p_i)^{(1-y)}$

#### Dowód 

1. $E(Y_i|X = x) = P(Y_i = 1) * 1 + P(Y_i = 0) * 0 = P(Y_i = 1) = p_i$
2. $P(Y_i = 1 | X =x) = p_i^1 (1 - p_i)^{(1 - 1)} = p_i$
   $P(Y_i = 0 | X =x) = p_i^0 (1 - p_i)^{(1 - 0)} = 1 - p_i$

## Dlaczogo funkcja logit?

Chcemy stworzyć model który powie nam czy pod warunkiem $X = x_i$ realizacja zmiennej $Y$ zakończy się sukcesem (1) czy porażkom (0). Nie wiemy jak to zrobić ale jesteśmy całkiem nieźli w modelach liniowych ;) .

Tworzymy zatem model liniowy 

$$ Y_i = \beta X_i $$
Jest prawie dobrze ale $Y_i$ jako kombinacja liniowa zmiennych, które przeważnie nie są ograniczone, może przyjąć dowolnie duże (małe) wartości, w szczegulności różne od $\{0, 1\}$ .  

## Dlaczogo funkcja logit? II

Nic straconego!!! W liceum poznaliśmy funkcje monotoniczne, użyjmy ich! 

$$
f(x) = \{
 \begin{array}{cc}
1 & jeżeli \ x > 0  \\
0 & jeżeli \ x < 0
\end{array}
$$
```{r required_packages, include=FALSE, eval=TRUE}
require('ggplot2')
require("GGally")
require("dplyr")
setwd("D:/GIT/sg-lectures")
```

```{r ex_function, echo=FALSE, fig.height=3.5}
x <- seq(-10, 10, by = 0.1)
f_x <- ifelse(x > 0, 1, 0)
dt1 <- data.frame(x_val = x, y_val = f_x)
ggplot(data = dt1, aes(x_val, y_val)) + geom_point(colour = rgb(1,  0.463, 0.165))
```

## Dlaczogo funkcja logit? III

udało się! wystarczy przyłożyć funkcję $f$ do prawej strony róWnania 
$$ Y_i = \beta X_i $$

i otrzymujemy 

$$ Y_i = f(\beta X_i) = \{
 \begin{array}{cc}
1 & jeżeli \ \beta X_i > 0  \\
0 & jeżeli \ \beta X_i < 0
\end{array} $$

## Dlaczogo funkcja logit? IV

#### Problemy 

1. co gdy $\beta X_i = 0$ (z tym łątwo sobie poradzić zmieniając jedną z nierówności na słabom)
2. funkcja jest nieciągła i nieróżniczkowalna co sprawia wiele problemów formalnych. Przykłądowo gdy chemy wyznaczyć wspułćzynniki $\beta$ to co robimy to maksymalizujemy funkcję wiraygodności 
  $$
  L(\beta) = \prod_{i: \ y_i = 1}p(x_i)\prod_{i: \ y_i = 0}(1 - p(x_i))
  $$
ale jest ona nierużniczkowalna, więc nie możemy znaleźć analitycznie maksimum!

## Dlaczogo funkcja logit? IV cd
<ol start = 3>
<li>
funkcja jest idelana do problemów liniowo separowalnych to co tak napradę zrobiliśmy to poprowadziliśmy hiperpłaszczyznę i powiedzieliśmy wszystko z jednej strony jest sukcesem a z drugiej poraszką. Jednak w żeczywistości mało który problem jest liniowo separowalny (pomysł nie jest zupełnie chybiony [perceptron](https://en.wikipedia.org/wiki/Perceptron).
</li>
</ol>
## Dlaczogo funkcja logit? V

#### Definicja (Logit i funkcja odwrotna do logit)

1. logit: $logit(p) = ln(\frac{p}{1-p}) = ln(odds)$
2. funkcja odwrotna $p = \frac{1}{1 + \exp(-odds)}$


## Dlaczogo funkcja logit? VI
```{r logit, echo=FALSE}
s <- seq(0, 1, by = 0.001)
logit <- data.frame(x = s, y = log(s / (1 - s)))
ggplot(data = logit, aes(x = x, y = y)) + geom_point(colour = rgb(1,  0.463, 0.165))
```


## Dlaczogo funkcja logit? VII
```{r r_logit, echo=FALSE}
s_x <- seq(-10, 10, by = 0.1)
s_y <- 1 / (1 + exp(-s_x))
logit <- data.frame(x = s_x, y = s_y)
ggplot(data = logit, aes(x = x, y = y)) + geom_point(colour = rgb(1,  0.463, 0.165))
```

## Dlaczogo funkcja logit? VIII

Użyjmy funkcj logit do naszego problemu 

$$
 logit(Y) = \beta X
$$
zatem 
$$
 Y = \frac{e^{\beta X}}{1 + e^{\beta X}}
$$

## Dlaczogo funkcja logit? IX

#### Brak problemów 
1. $\beta X_i = 0$ wóWczas $\frac{e^{0.5}}{1 + e^{0.5}} = 0.5$
2. funkcja jest ciągła monotoniczna i jest klasy $C^\infty$
3. Poprowadziliśmy idealnie taką samą hiperpłąszczyznę co wczejśniej, ale teraz, w zależności od tego po której stronie hiperpłaszczyzny znajdzie się obserwacja i w jakiej odległości przypisujemy jej wartość od 0 do 1 co naturalnie można utożsamiać z prawdopodobieństwem sukcesu (bo $E(Y| X = x) =p$).

## alternatywy do funkcji logit
```{r r_alt, echo=FALSE}
x <- seq(-10, 10, by = 0.1)
probit <- pnorm(x)
logit <- 1 / (1 + exp(-x))
th <- tanh(x) / 2 + 0.5
dt <- data.frame(x_val = rep(x, 3), y_val = c(probit, logit, th), label = c(rep("probit", 201), rep("logit", 201), rep("tanh(x) / 2 + 0.5", 201)))

ggplot(dt, aes(x_val, y_val, colour = label)) + geom_line()

```

## Regresja w R

```{r log_reg}
# GGally

cr <- mtcars
cr$am <- as.factor(cr$am)
ggpairs(cr, columns = c("mpg", "cyl", "disp", "hp"), 
        mapping = ggplot2::aes(colour = am)
        )

```

# przygotowanie próby
```{r train_test}
train <- sample_frac(cr, 0.6)
test <- anti_join(cr, train)

table(train$am) / nrow(train)

table(test$am) / nrow(test)


```