---
title: "DecisionTrees"
author: "Mateusz Sobieraj"
date: "31 maja 2018"
output: 
  ioslides_presentation:
    css: dependencies/styles.css
    logo: dependencies/grosz.png
logo: dependencies/grosz.png
smaller: yes
encoding: UTF-8
transition: slower
widescreen: no
---

```{r setup, include=FALSE}
library(dplyr)
library(ggplot2)
```

## Wstęp

Oznaczenia

- $X$ - dowolna zmienna losowa
- $P$ - miara probabilistyczna
- $n$ - liczba obserwacji
- $x_i$ - i-ta obserwacja
- $p_i$ - prawdopodobieństwo $x_i$
- $S$ - zbiór
- $|S|$ liczność zbioru

## Wstęp

#### DEF Algorytm zachłanny

Algorytm który w celu wyznaczenia rozwiązania, w każdym kroku podejmuje najlepszą lokalnie decyzję.

## Wstęp

#### DEF Informacja wzajemna

Informację wzajemną zdefiniujmy wzorem

$$
I(X) = \log_b\frac{1}{P(X)}
$$

## Wstęp

####  UWAGA 

Informacja wzajemna jest to wartość informacyjna zajścia zdarzenia.

## Entropia

#### DEF Entropia

Entropię zmiennej $X$ zdefiniujemy wzorem

$$
H(X) = \mathbb{E}(I(X)) = \mathbb{E}(\log_b\frac{1}{P(X)})
$$

## Entropia

#### DEF Estymator entropii

Estymator entropi zmiennej X zdefiniujemy wzorem 

$$
\hat{H}(X) = \sum_{i = 1}^n p_i \log_b\frac{1}{p_i}
$$

## Entropia

####UWAGA 

Łatwo zauważyć, że entropia to średnia ważona informacji wzajemnej poszczególnych zdarzeń,
ważona prawdopodobieństwem.

## Entropia

####UWAGA 

Gdy mamy zbiór danych $S$, który zawiera $k$ kategorii, wówczas

- prawdopodobieństwo kategorii $j$ wynosi $p_j = \frac{liczba\ wystąpień\ kategorii\ j}{liczność\ zbioru}$
- Entropia w Zbiorze wynosi $\hat{H}(S) = \sum_{j = 1}^k p_j \log_b\frac{1}{p_j}$

## Entropia

Implementacja entropii w $R$.

```{r entropy}
entropy <- function(x, b = 2) {
  probs <- x %>% table() %>% prop.table()
  self_information <- log(1/ probs, base = b)
  sum(probs * self_information)
}
```

## Entropia

```{r}
entropy(c(1, 0, 0, 1))
entropy(c(0, 0, 0, 0))
entropy(c(0, 1, 1, 1))

```

## Entropia

Wykres entropii dla dwóch kategorii

```{r echo=FALSE}
tibble(p1 = seq(0.02, 0.98, by = 0.02)) %>% 
  mutate(p2 = 1 - p1) %>% 
  mutate(entropy = -p1 * log2(p1) - p2 * log2(p2)) %>% 
  ggplot(aes(p1, entropy)) +
  geom_point(colour = "#ff9060", size = 2)
```


## Zysk informacyjny (Information gain)

Rozważmy

- $S$ - zbiór danych
- $P = \{S_1, \dots, S_k\}$ - podział $S$

Wówczas zysk informacyjny $IG$ definiujemy wzorem

$$
IG(S, P) = H(S) - \sum_{i=1}^k \frac{|S_i|}{|S|} H(S_i)
$$


## Zysk informacyjny (Information gain)

#### UWAGA 

Łatwo zauważyć, że 

- $\frac{|S_i|}{|S|}$ to prawdopodobieństwo należenia do zbioru $i$.
- $H(S)$ jest entropią całego układu
- $H(S_i)$ jest entropią wewnątrz zbioru. W naszym przypadku liścia. Ponieważ chcemy
mieć jak najczystsze liście więc im niższa tym lepiej
- $IG(S,P)$ mierzy redukcję entropii układu, spowodowaną podziałem $P$
- chcemy zatem maksymalizować $IG(S,P)$ po $P$

## Zysk informacyjny (Information gain)

#### UWAGA

Jeżeli $P$ jest podziałem tylko na podzbiory czyste (posiadające tylko jedną kategorię) wówczas dla
$i = 1 \dots k$, $S_i = 0$ zatem redukcja entropi wynosi $IG(S, P) = H(S)$, a entropia
nowego układu równa się $0$.



## Zysk informacyjny (Information gain)

#### UWAGA 

Zysk informacyjny nazywany jest też entropią względną lub dywergencją Kullbacka-Leiblera.

## Zysk informacyjny (Information gain)

#### UWAGA 

Zysk informacyjny jest wrażliwy na liczbę kategorii, Jeżeli rozważam zmienne z różnymi liczbami
kategorii, lepiej stosować $gain ratio$.

## Zysk informacyjny (Information gain)

#### DEF Split information

Split information zdefiniujemy wzorem

$$
SI(S, P) = \sum_{i=1}^k \frac{|S_i|}{|S|}\log_b\frac{|S|}{|S_i|} 
$$

#### DEF Gain ratio

Gain Ratio zdefiniujemy wzorem

$$
GR(S, P) = \frac{IG(S,P)}{SI(S, P)}
$$

## Algorytm ID3 oznaczenia

Zaktualizujmy oznaczenia do problemu klasyfikacji

- $\mathbb{X}$ - macież zmiennych objaśniających
- $Y$ - zmienna objaśniana
- $k$ - liczba kategorii zmiennej objaśnianej
- $p_i$ - prawdopodobieństwo wystąpienia i-tej kategorii


## Algorytm ID3 uproszczenia

#### UWAGA

Oryginalnie algorytm $ID3$, został stworzony przez Rossa Quinlana i w tej wersji,
dopuszczał podział na dowolnie wiele kategorii. W naszej uproszczonej implementacji
będziemy rozważać tylko drzewa binarne. Przyniesie nam to następujące korzyści

- możemy posługiwać się zyskiem informacyjnym $IG$
- łatwo dostosować algorytm do zmiennych ciągłych

## Algorytm ID3 zarys

Schemat algorytmu:

1. Weź dane zawierające zmienną objaśnianą i zmienne objaśniające
2. Wybierz najlepszą zmienną
3. Dokonaj podziału po wybranej zmiennej
4. Rekurencyjnie powtarzaj powyższe, aż do warunku stopu


## 2. Wybierz najlepszą zmienną

1. Dla wszystkich zmiennych objaśniających
    - posortuj (dla zmiennych kategorialnych nie posiadających relacji porządku mogę sprawdzić
    wszystkie kombinacje kategorii, lub rozkodować zmienną na zmienne binarne)
    - idąc od lewej do prawej licz zysk informacyjny dla każdej zmiennej
2. Wybierz zmienną o największym zysku informacyjnym


## Algorytm ID3 Warunek stopu

1. Idealny podział (w każdym liściu są wartości jednej klasy)
2. Nie pozostało więcej zmiennych do tworzenia podziału (ID3 używa każdej zmiennej tylko raz)

## Alogrytm ID3 Overfitting 

#### UWAGA

Ponieważ warunek stopu jest bardzo liberalny, zatem algorytm jest podatny na przetrenowanie.

## Alogrytm ID3 Overfitting 

#### UWAGA

Popularnym sposobem radzenia sobie z przetrenowaniem drzewa jest porównywanie jego 
wartości predykcyjnej na zbiorze walidacyjnym lub kroswalidacyjnie (log-loss, accuracy itp.).

Stosuje się dwa podejścia

- buduje drzewo do momentu, aż wartość predykcyjna na zbiorze walidacyjnym zaczyna spadać
- buduje pełne drzewo i przycinam (prune) je do momentu, aż wartość predykcyjna na zbiorze
walidacyjnym zaczyna spadać.

