---
title: "XGBoost"
author: "Mateusz Sobieraj"
date: "19 maja 2018"
output: 
  ioslides_presentation:
    css: dependencies/styles.css
    logo: dependencies/grosz.png
logo: dependencies/grosz.png
smaller: yes
encoding: UTF-8
transition: slower
widescreen: no
---

```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(ggplot2)
library(GGally)
library(igraph)
library(ggnet)
library(ggplot2)
```

## Przygotowanie środowiska 

```{r}
library(xgboost)
data(agaricus.train, package = 'xgboost')
data(agaricus.test, package = 'xgboost')
train <- agaricus.train
test <- agaricus.test
```

## opis danych


```{r}
```


## Oznaczenia 

- $d$ - liczba zmiennych
- $\mathbb{x}_i \in \mathbb{R}$ - i-ty wektor miennych objaśniających
- $y_i$ - i-ta wartość zmiennej objeśnianej
- $\hat{y_i}$ - i-ta predykcja
- $\Theta$ - zbiór parametrów modelu
- $L(\Theta)$ - funkcja straty
- $\Omega(\Theta)$ - regularyzacja
- $O(\Theta) = L(\Theta) + \Omega(\Theta)$  - funckja celu

## Drzewa daecyzyjne

<img src = "dependencies/tree1.png" width="75%" height="75%"/>

## Drzewa daecyzyjne

- niezależne od skali zmiennych 
- łatwo skalowalne
- uczą się ogólnych zależności między cechami
- łatwo implementowalne
- jeżeli płytkie, wówaczas mają małą wariancję

## Założenia XGboost

Model definiujemy w następujący sposób

$$
\hat{y_i} = \sum_{k = 1}^K f_k(x_i), \quad f_k \in  \mathfrak{F}
$$
gdzie

 - $K$ - liczba drzew
 - $f_k$ - k-te drzewo
 - $\mathfrak{F}$ przestrzeń funkcyjna drzew decyzyjnych

Przestrzeń parametrów

- $\Theta = \{f_1, f_2, \dots, f_K\}$

## Funkcja celu

$$O(\Theta ) = \sum_{i = 1} ^ n l(y_i, \hat{y_i}) + \sum_{k = 1}^K \Omega(f_k)$$
gdzie

- $n$ - liczba obserwacji

Przykłady zdefiniowania $\Omega$

- Liczba gałęzi
- głębokość drzewa
- wartość liści

## Uczenie (Aditive Training)

$$
\hat{y}_i^{(0)} = 0 \\
\hat{y}_i^{(1)} = f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i) \\
\hat{y}_i^{(2)} = f_1(x_i) + f_2(x_i) = \hat{y}_i^{(1)} + f_2(x_i) \\
\dots \\
\hat{y}_i^{(K)} = \sum_{k = 0}^K f_k(x_i) = \hat{y}_i^{(K-1)} + f_K(x_i)
$$
gdzie $\hat{y}_i^{(\_)}$ - model w danym momencie treningu dla obserwacji $i$.

## Uczenie wybór następnego drzewa w algorytmie I

Rozważmy moment $t$. W tym momencie nasz algorytm wygląda następująco

$$
\hat{y}_i^{(t)} =  \hat{f}_i^{(t-1)} + f_t(x_i)
$$

Szukamy jak najlepszego $f_t$.

## Uczenie wybór następnego drzewa w algorytmie II

Nasza funkcja celu w momencie $t$ jest postaci:

$$
  O^t(\Theta ) = \sum_{i = 1} ^ n l(y_i, \hat{y_i}^t) + \sum_{k = 0}^t \Omega(f_k) = \\
  \sum_{i = 1}^n l(y_i, \hat{y}_i^{t-1} + f_t(x_i)) + \Omega(f_t) + c \rightarrow min
$$

gdzie

- $c$ - stała wynikająca z regularyzacji we wcześniejszych korkach, nie mająca wpływu
na obecnym etapie

## Uczenie wybór następnego drzewa w algorytmie II

Naszym celem jest zminimalizowanie $O$ po $f_t$.

$$
  O^t(\Theta ) = \sum_{i = 1} ^ n (y_i -  (\hat{y_i}^{t-1} + f_t(x_i)))^2 + \Omega(f_t) + c_1 = \\
  \sum_{i = 1} ^ n [2(y_i -  \hat{y_i}^{t-1})f_t(x_i) + f_t(x_i)^2 + (y_i -  \hat{y_i}^{t-1}) ^2] + \Omega(f_t) + c_1 = 
$$

## Uczenie wybór następnego drzewa w algorytmie II

Ponieważ $(y_i -  \hat{y_i}^{t-1}) ^2$ to stała policzona w poprzednim kroku możemy zapisać powyższe jako

$$
\sum_{i = 1} ^ n [2(y_i -  \hat{y_i}^{t-1})f_t(x_i) + f_t(x_i)^2] + \Omega(f_t) + c_2
$$

## Wzór Tailora

$$ f(x + \Delta x) 	\approx	 f(x) + \Delta x f'(x) + \frac{\Delta x ^2}{2} f''(x)$$
Oznaczmy 

- $g_i = \partial_{\hat{y}^{t-1}}l(y, \hat{y}^{t-1})$
- $h_i = \partial_{\hat{y}^{t-1}}^2 l(y, \hat{y}^{t-1})$

## Wzór Tailora

WóWczas dla błędu kwadratowego mamy

$$
  g_i = \partial_{\hat{y}^{t-1}}l(y, \hat{y}^{t-1}) = 
  \partial_{\hat{y}^{t-1}} (y_i -  \hat{y_i}^{t-1})^2 =
  2 (y_i -  \hat{y_i}^{t-1})
$$

oraz

$$
  h_i = \partial_{\hat{y}^{t-1}}^2 l(y, \hat{y}^{t-1}) = \partial_{\hat{y}^{t-1}} g_i =
  \partial_{\hat{y}^{t-1}} 2 (y_i -  \hat{y_i}^{t-1}) = 2
$$


- $h_i = \partial_{\hat{y}^{t-1}}^2 l(y, \hat{y}^{t-1})$

## Wykożystanie wzoru taylora

Potraktujmy funkcję straty $l$ jako funkcję jednego (drugiego) argumentu $\hat{y}$

Oraz trzymając się notacj ze wzrou Talora

- $f(x)$ = $l(y_i, \hat{y}_i^{t-1})$
- $\Delta x$ = $f_t(x_i)$

Wówczas po podstawieniu otrzymujemy


$$ 
l(y_i, \hat{y}_i^{t-1} + f_t(x_i)) \approx l(y_i, \hat{y}_i^{t-1}) + f_t(x_i)g_i + \frac{1}{2}f_t(x_i)^2h_i
$$

## Wykożystanie wzoru taylora

A zatem 

$$
O^t = \sum_{i = 1}^n [l(y_i, \hat{y}_i^{t-1}) + f_t(x_i)g_i + \frac{1}{2}f_t(x_i)^2h_i] + \Omega(f_t) + c_1
$$

## Wykożystanie wzoru Taylora 

oraz dla błędu kwadratowego

$$
  O^t = \sum_{i = 1}^n [l(y_i, \hat{y}_i^{t-1}) + 2 (y_i - \hat{y_i}^{t-1}) f_t(x_i) + \\
  f_t(x_i)^2] + \Omega(f_t) + c_1
$$

## Nowa funkcja celu

Po usunięciu stałych otrzymujemy

$$
O^t \approx \sum_{i = 1}^n [f_t(x_i)g_i + \frac{1}{2}f_t(x_i)^2h_i] + \Omega(f_t)
$$

oraz dla błędu kwadratowego

$$
O^t \approx \sum_{i = 1}^n [2 (y_i - \hat{y_i}^{t-1}) f_t(x_i) + f_t(x_i)^2] + \Omega(f_t)
$$

## Definicja drzewa

Niech

- $f_t: \mathbb{R}^n \rightarrow \mathbb{R}^T$ - t-te drzewo
- $x \in \mathbb{R}^n$ - obserwacja (wektor n zmiennych)
- $w \in \mathbb{R}^T$ - wektor wag (scoringow)
- $q: \mathbb{R}^d \rightarrow \{1 \dots T\}$ - struktóa drzewa (to co widzimy jako ify)

Wówczas drzewo definiujemy następująco

$$
f_t(x) = w_{q(x)}
$$

## Definicja drzewa

Przykład

Spróbujmmy przedstawić poniższe drzewo w przedstawionej w poprzednim slaidzie notacji.

<img src = "dependencies/tree_def_example.png" width="75%" height="75%"/>

## Definicja drzewa

- $w = [0.2, 0.15, 0.5]$
- $T = [1, 2, 3]$
- $q(x) = I_{balance <= 1000} * 3 +  \\  I_{balance > 1000} (I_{is\_young = 1} + I_{is\_young = 0} * 2)$

## Definicja złożoności drzewa

Złożoność dżewa zdefiniujmy następująco

$$
\Omega(f_t) = \gamma T + \frac{1}{2} \lambda \sum_{j = 1}^T w_j^2
$$
Uwaga
Karzemy jednocześnie liczbę liści jaki i wielkość współczynników.


## Powrót do funkcji celu 

Zdefiniujmy zbiór wszystkich indeksów, należących do liścia $j$: 

$$I_j = \{i|q(x_i) = j\}$$

Ponieważ, każda pobserwacja jest jednoznacznie przypożądkowana do liścia, zachodzi następująca róWność

$$\sum_{i = 1}^n x_i = \sum_{i = j}^T \sum_{i \in I_j} x_i$$

## Powrót do funkcji celu 

Nasza obecna forma funkcji celu jest następująca

$$
O^t \approx \sum_{i = 1}^n [f_t(x_i)g_i + \frac{1}{2}f_t(x_i)^2h_i] + \Omega(f_t) = \\
\sum_{i = 1}^n [f_t(x_i)g_i + \frac{1}{2}f_t(x_i)^2h_i] + \gamma T + \frac{1}{2} \lambda \sum_{j = 1}^T w_j^2 =
$$

Następnie z definicj drzewa $f_t(x) = w_{q(x)}$

$$
\sum_{i = 1}^n [w_{q(x_i)}g_i + \frac{1}{2}w_{q(x_i)}^2h_i] + \gamma T + \frac{1}{2} \lambda \sum_{j = 1}^T w_j^2
$$
oraz z poprzedniej własności, oraz ponieważ w każdym liściu waga $w_\_$ jest taka sama mamy

## Powrót do funkcji celu 

$$
\sum_{i = j}^T [(\sum_{i \in I_j} g_i) w_j + \frac{1}{2}(\sum_{i \in I_j} h_i)w_j^2] +
\gamma T + \frac{1}{2} \lambda \sum_{j = 1}^T w_j^2 =\\
\sum_{i = j}^T [(\sum_{i \in I_j} g_i) w_j + \frac{1}{2}(\sum_{i \in I_j} h_i + \lambda)w_j^2] + \gamma T
$$

Otrzymaliśmy sumę $T$ równań kwadratowych po $w_\_$.

## Własności funkcji kwadratowej

Niech

$$
f(x) = \frac{1}{2} a x ^ 2 + b x + c \quad c > 0
$$
Własności 1

$$argmin_x(f) = - \frac{b}{a}$$
Istotnie 

$$
f`(x) = ax + b \\
f`(x) = 0 \iff ax + b = 0 \iff x = - \frac{b}{a}
$$

## Własności funkcji kwadratowej

Własność 2

$$\min(f) = - \frac{1}{2}\frac{b^2}{a} + c$$
Istotnie 

$$
f(argmin_x(f)) = \frac{1}{2} a argmin_x(f) ^ 2 + b argmin_x(f) + c = \\
\frac{1}{2} a (- \frac{b}{a}) ^ 2 + b (- \frac{b}{a}) + c = \\
\frac{1}{2} \frac{b^2}{a} - \frac{b^2}{a} + c  =  - \frac{1}{2}\frac{b^2}{a} + c
$$

## Powrót do funkcji celu 

Niech:

- $G_j = \sum_{i \in I_j} g_i$
- $H_j = \sum_{i \in I_j} h_i$

Wówczas funkcja celu przyjmuje postać

$$
\sum_{i = j}^T [G_j w_j + \frac{1}{2}(H_j + \lambda)w_j^2] + \gamma T
$$

## Powrót do funkcji celu 

Załóżmy teraz, żę struktura drzewa jest $q(x)$ jeststała, wówczas optymalne wagi
z własności funkcji kwadratowej są postaci $w_j^* = \frac{G_j}{H_j + \lambda}$. 
Oraz wartość funkcji celu 

$$
O^t = -\sum_{i = j}^T \frac{G_j^2}{H_j + \lambda} + \gamma T
$$
Uwaga 

$\sum_{i = j}^T \frac{G_j^2}{H_j + \lambda}$ ta wartość mówi jak dobra jest struktóa naszego drzewa

## Przykład

Załóżmy, że mamy dżewo z trzema liśćmi. 

liść                        |obserwcje w liściu|wartości parametrów
----------------------------|------------------|-------------------
balance > 1000, is_young = T|1, 6              |$I_1 = \{1, 6\}$, $G_1 = g_1 + g_6$, $H_1 = h_1 + h_6$
balance > 1000, is_young = F|2, 3, 4           |$I_2 = \{2, 3, 4\}$, $G_2 = g_2 + g_3 + g_4$, $H_2 = h_2 + h_3 + h_4$
balance <= 1000             |5                 |$I_3 = \{5\}$, $G_3 = g_5$, $H_3 = h_5$

## Znjadowanie najleprzego drzewa podejście naiwne

- Iteruj po wszyskich możliwych struktóach drzew $q$
- Dla każdego $q$ policz funkcję celu $O^t$

$$
O^t = -\sum_{i = j}^T \frac{G_j^2}{H_j + \lambda} + \gamma T
$$

- Weź to tą struktórę któa ma najmniejszą wartość funkcji celu

$w_j^* = \frac{G_j}{H_j + \lambda}$

- podejście jest słabe, jest bardzo dużo możliwych struktur

## Znjadowanie najlepszego drzewa w praktyce

- zaczynamy od drzewa głębokości 0
- dla każdego dżewa liczymy


$$
Gain = [\frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} -
\frac{(G_R + G_H)^2}{H_R + H_L + \lambda}] - \gamma
$$
- wystarczy posortować instancje i policzyć $Gain$ idąc od lewej do prawej

## Znjadowanie najlepszego drzewa w praktyce

Uwaga 

We wzorze na $Gain$ cztery komponenty to odpowiednio score lewego liścia, 
score prawego liści, scor łączny, oraz koszt złożoności wynikający z dodatkowego liścia.
Oczywiści podział warto rozważać tylko wtedy gdy $Gain$ jest dodatnie.

## Przykład

Rozważmy zmienną $balance$, posortowaną rosnąco

balance   |104  |1002 |a|1432 |1545 |1789 |1800
----------|-----|-----|-|-----|-----|-----|----
obserwacja|5    |1    |-|6    |4    |2    |3
g         |$g_5$|$g_1$|-|$g_6$|$g_4$|$g_2$|$g_3$
h         |$h_5$|$h_1$|-|$h_6$|$h_4$|$h_2$|$h_3$

Znajdziemy $Gain$ dla podziału $a$.

## Przykład

Mamy 

- $G_L = g_5 + g_1$
- $H_L = h_5 + h_1$
- $G_R = g_6 + g_4 + g_2 + g_3$
- $H_R = h_6 + h_4 + h_2 + h_3$

Zatem

$$
Gain = [\frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} -
\frac{(G_R + G_H)^2}{H_R + H_L + \lambda}] - \gamma
$$

## Podsumowanie - algorytm znajdowania najlepszego splita

```r
  for wezęzeł in wszystkie węzły:
    for zmienna in wszystkie zmienne:
      posortuj obserwacje po wartościach aktualnej zmiennej
      licz gain od lewej do prawej by znaleźć nailepszy podział
      weź najlepszy podział z wszystkich cech
```
## Zmienne kategoryczne

- one-hot encoding
- przypisanie każdej kategorii cyfry (tracimy część inforamcji)

## Nauka drzewa dla jednej zmiennej

Niech 

- $x$ - wiek klienta w latach
- $y$ - score

```{r echo=FALSE}
df <- tibble(
  wiek = seq(from = 20, to = 80, by = 5),
  score = c(.1, .1, .1, .15, .3, .5, .58, .6, .6, .58, .57, .6, .6)
)

p1 <- df %>% 
  ggplot(aes(wiek, score)) + 
  geom_point(colour = "#ff9060", size = 3)
p1
```

## Nauka drzewa dla jednej zmiennej

- $L$ - jak dobrze drzewo (funkcja) pasuje do punktów
- $\Omega$ 
    - liczba punktów podziału?
    - wysokość każdego segmentu (wielkość wspułycznników)?

## Nauka drzewa dla jednej zmiennej

Zbyt dużo podziałów $\Omega(f)$ duża

```{r echo=FALSE}
df2 <- tibble(
  wiek = df$wiek,
  score = df$score
)

df2 %>% 
  ggplot(aes(wiek, score)) +
  geom_col(fill = "#ff9060", alpha = 0.5, width = 5, colour = "#ff9060") +
  geom_point(colour = "#ff9060", size = 3, data = df)

```


## Nauka drzewa dla jednej zmiennej

Zły punkt podziału $L(f)$ duże

```{r echo=FALSE}
df2 <- tibble(
  wiek = c(40, 80),
  score = c(0.4, 0.6)
)

df2 %>% 
  ggplot(aes(wiek, score)) +
  geom_col(fill = "#ff9060", alpha = 0.5, width = 40, colour = "#ff9060") +
  geom_point(colour = "#ff9060", size = 3, data = df)

```

## Nauka drzewa dla jednej zmiennej

Dobry podział

```{r echo=FALSE}
df2 <- tibble(
  wiek = c(30, 80),
  score = c(0.3, 0.6)
)

df2 %>% 
  ggplot(aes(wiek, score, width = wiek - 5)) +
  geom_col(fill = "#ff9060", alpha = 0.5, colour = "#ff9060") +
  geom_point(colour = "#ff9060", size = 3, data = df)

```

## Źródła

- http://xgboost.readthedocs.io
- https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf
- https://archive.ics.uci.edu/ml/datasets/Mushroom

