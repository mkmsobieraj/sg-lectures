---
title: "Wykład VI - caret overview"
author: "Mateusz Sobieraj"
date: "10 maj 2018"
output: 
  ioslides_presentation:
    css: dependencies/styles.css
    logo: dependencies/grosz.png
logo: dependencies/grosz.png
smaller: yes
encoding: UTF-8
transition: slower
widescreen: no
---

```{r required_packages, include=FALSE, eval=TRUE}
library(MASS)
library(dplyr)
library(tidyr)
library(ggplot2)
library(GGally)
library(caret)
library(randomForest)
Boston <- Boston %>% 
  mutate(chas = as.factor(chas))
detach("package:MASS", unload = TRUE)
iris_con <- iris %>% select(-Species)
iris_disc <- iris %>% select(Species) 
fly <- read.csv("dependencies/FlightDelays.csv")
fly <- fly %>%
  select(- flightnumber, -tailnu, -date, -dest) %>%
  mutate(delay = ifelse(delay == "delayed", 1, 0) %>% as.factor())
```



## Wizualizacja ggplot zamiast caret I

Wizualizacja danych z pomocą `caret` jest łatwa do zastąpienia, przez użycie 
bardziej elastycznego rozwiązania z `ggplot2`.

  
## Wizualizacja ggplot zamiast caret I


Najpierw tworzymy funkcję, która jest wraperem dla funkcji `ggplot`.

```{r prepare_my_plot}
my_plot <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) +
    geom_point(...) +
    geom_smooth(method = lm,
                se = FALSE,
                linetype = 2,
                color = "gray50") +
    stat_ellipse(type = "norm")
    
}
```

## Wizualizacja ggplot zamiast caret I

Następnie używamy jej by wyrysować na dolnej macierzy zmienne ciągłe.

```{r pairs_code}
iris_pairs <- iris %>% 
  ggpairs(aes(fill = Species,
              colour = Species,
              alpha = .5,
              shape = Species),
          columns = 1:4,
          upper = list(continuous = "cor",
                       combo = "box",
                       discrete = "blank"),
          lower = list(continuous  = my_plot),
          diag = list())
```

## Wizualizacja ggplot zamiast caret I

```{r pairs_plot, echo=FALSE}
iris_pairs
```

## Wizualizacja ggplot zamiast caret II 

Rysowanie wykresów gęstości jest jeszcze prostrze.

```{r density_code}
iris_density <- iris %>%
  gather(key = "faeture", value = "value", -Species) %>% 
  ggplot(aes(value, colour = Species, linetype = Species)) + 
  geom_density() +
  geom_rug() +
  facet_wrap(~faeture, nrow = 1) +
  theme(legend.position = "top", legend.box = "horizontal")
```

## Wizualizacja ggplot zamiast caret II 

```{r density_plot, echo=FALSE}
iris_density
```

## Wizualizacja ggplot zamiast caret III

A teraz box plot ...

```{r box_code}
iris_box <- iris %>%
  gather(key = "faeture", value = "value", -Species) %>% 
  ggplot(aes(Species, value)) + 
  geom_boxplot() +
  stat_summary(fun.y = mean, geom = "point", size = 2, colour = "red") +
  facet_wrap(~faeture, nrow = 1) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Wizualizacja ggplot zamiast caret III

```{r box_plot, echo=FALSE}
iris_box
```

## Wizualizacja ggplot zamiast caret IV

i scatter plot.

```{r scatter_code}
iris_scatter <- iris %>%
  ggplot(aes(Sepal.Length, Petal.Length)) + 
  geom_point(alpha = .5, colour = "#000000") +
    geom_smooth(method = "gam",
              se = FALSE,
              color = "#ff9060") +
  facet_wrap(~Species, nrow = 1)

```

## Wizualizacja ggplot zamiast caret IV

```{r scatter_plot, echo=FALSE}
iris_scatter
```



## Tworzenie zmiennych 0-1 z `dummyVars`

Zmienne binarne tworzymy podając odpowiednią formułe...

```{r}
iris %>% 
  dummyVars(Sepal.Length ~ Species, data = .) %>% 
  predict(newdata = iris) %>% 
  as_tibble()
```

## Tworzenie zmiennych 0-1 z `dummyVars`

... funkcja `dummyVars` działa bardzo podobnie do wbudowanej w `R` funkcji `model.matrix`.

```{r}
iris %>%
  model.matrix(Species ~ . ^ 2, data = .) %>% as_tibble() %>% 
  as_tibble() %>% head()
```


## `findCorrelation` I

Za pomocą `findCorrelation` możemy łatwo znaleść zmienne o zbyt dużej korelacji i 
je wyżucić.

```{r}
corelation <- cor(iris_con)
findCorrelation(corelation, cutoff = 0.5)
```

## `findCorrelation` II

Alternatywnie możemy napisać własny krutki skrypt.

```{r}
mask <- lower.tri(corelation)
cor_df <- ifelse(mask, corelation, NA)
colnames(cor_df) <- colnames(corelation)
rownames(cor_df) <- rownames(corelation)

cor_df <- cor_df %>% 
    as_tibble(rownames = "var1") %>% 
    gather(key = "var2", value = "corelation", -var1) %>% 
    filter(!is.na(corelation))
    
cor_df %>% 
  filter(abs(corelation) > 0.5) 
```


## Dzielenie danych `createDataPartition` I

Zalety `createDataPartition` to

- wygoda
- dane podzielone sa równomiernie
- sa zachowane proporcje w zbiorach

Wady 

- nie da się podzielić zbioru na trzy części (łatwo obejść dzieląc jeden ze zbiorów dwa razy)

## Dzielenie danych `createDataPartition` I

```{r}

index <- createDataPartition(iris$Species, p = .6, list = F)

train <- iris %>% slice(index)
train_con <- iris_con %>% slice(index)
test <- iris %>% slice(-index)
test_con <- iris_con %>% slice(-index)
```


## Dzielenie danych `createDataPartition` II

Proporcje kategorii w zbiorze treningowym są idealne ...

```{r}
table(train$Species)
```

## Dzielenie danych `createDataPartition` II

podobnie jak w zbiorze testowym.

```{r}
table(test$Species)
```

## Dzielenie danych `createDataPartition` III

Możemy szybko napisać skrypt który losjuje z zadaną przez nas częstością ...

```{r}
set.seed(100)
categories <- iris %>% 
    select(Species) %>%
    pull() %>%
    length() %>% 
    sample(1:3, ., replace = TRUE, prob = c(0.6, 0.3, 0.1))

indexes <- 
  sapply(1:3, function(flag, categories){
    which(categories %in% flag)  
  }, categories)

names(indexes) <- c("train", "test", "validation")
```

## Dzielenie danych `createDataPartition` III

```{r, echo = FALSE}
indexes
```

## Dzielenie danych `createDataPartition` III

Ale zarówno liczności zbiorów ...

```{r, echo = FALSE}
indexes %>% sapply(length) / dim(iris)[1]
```


## Dzielenie danych `createDataPartition` III

jak i rozkład kategorii w grupach ...

```{r, echo= FALSE}
df_indexes <- stack(indexes)

df_indexes %>% 
  inner_join(iris %>% mutate(values = iris %>% rownames() %>% as.integer())) %>% 
  count(ind, Species) %>% 
  spread(Species, n)
```


## Dzielenie danych `createDataPartition` III

są przybliżone, a lepsza implementacja wymaga trochę więcej pracy.

## Przygotowanie danych z `preProcess` I

Zastosowania funkcji `preProcess`

- skalowanie
- centrowanie
- transformacje zmiennych (`BoxCox`, `YeoJohnson` itp.)

## Przygotowanie danych z  `preProcess` II

```{r}
preProcValues <- preProcess(train_con, method = c("center", "scale"))
predict(preProcValues, train_con) %>% head()
predict(preProcValues, test_con) %>% head()
```

## Krzywe diagnostyczne I przykładowe modele

Tworzę przykładowe modele do dalszej analizy

```{r}
index_b <- createDataPartition(Boston$chas, p = .6, list = F)
train_b <- Boston %>% slice(index_b)
test_b <- Boston %>% slice(-index_b)


m1 <- glm(formula = chas ~ ., data = train_b, family = binomial)
m2 <-  step(m1, direction = "forward", trace = FALSE)
m3 <- randomForest(formula = as.factor(chas) ~ ., data = train_b)

```

## Krzywe diagnostyczne I przygotowanie danych

Łącze ze sobą próbę testową i treningową, oraz dodaję prawdopodobieństwa sukcesu

```{r}


predictions <- bind_rows(
  train_b %>% mutate(type = "train")
          %>% mutate(prob = predict(m1, train_b, type = "response")),
  test_b  %>% mutate(type = "test")
          %>% mutate(prob = predict(m1, test_b, type = "response"))  
)

head(predictions)
```

## Krzywe diagnostyczne II hsiotgam I

```{r}
plot_hist <- predictions %>%
  ggplot(aes(prob, fill = type)) +
  geom_histogram(binwidth = .01) +
  facet_wrap(~chas + type)

```

## Krzywe diagnostyczne II hsiotgam I

```{r}
plot_hist
```


## Krzywe diagnostyczne III hsiotgam I

```{r}
plot_hist2 <- predictions %>%
  ggplot(aes(prob, fill = type)) +
  geom_histogram(binwidth = .01, alpha = 0.5) +
  facet_wrap(~chas)
```

## Krzywe diagnostyczne III hsiotgam II

```{r}
plot_hist2
```

## Miary jakości modelu I

Tworzę ramkę danych z podsumowaniem wyników.

```{r}
summary <- predictions %>% 
  filter(type == "test") %>% 
  rename(actual = chas) %>% 
  mutate(predicted = ifelse(prob > 0.3, 1, 0) %>% as.factor()) %>% 
  select(actual, predicted, prob)

summary %>% head()
```

## Miary jakości modelu II `confusionMatrix`


W łatwy sposób mogę otrzymać podsumowanie.

```{r}
cM <- confusionMatrix(data = summary$predicted, reference = summary$actual, positive = "1")
cM
```

## Miary jakości modelu II `confusionMatrix`

Mam łatwy dostęp do tabeli kondyngencji.

```{r}
cM$table 
```

## Miary jakości modelu II `confusionMatrix`

A także do każdej statsystyki.

```{r}
cM$overall
cM$overall[1]
```

## Miary jakości modelu III

Dla predykcji w któej występują tylko dwie klasy, mogę używać funckji 

- `twoClassSummary`
- `prSummary`

## Miary jakości modelu III

Uwaga, najwygodniej te funkcje stosuje się spełniając ich interface (przekazując 
im ramkę danych z opdopwiednimi nazwami kolumn)

```{r}
summary_cf <- summary %>% 
              mutate("failure" = 1 - prob) %>% 
              rename("succes" = prob,
                     obs = actual,
                     pred = predicted) %>% 
              mutate(obs = ifelse(obs == 1, "succes", "failure") %>% as.factor()) %>% 
              mutate(pred = ifelse(pred == 1, "succes", "failure") %>% as.factor()) %>% 
             as.data.frame()
summary_cf %>% head()
```

## Miary jakości modelu IV `twoClassSummary`

Wektor zwracany po użyciu `twoClassSummary` również daje mi łątwy dąstep do statystyk.

```{r}
tCS <- twoClassSummary(summary_cf, lev = levels(summary_cf$obs))
tCS
tCS[1]
```

## Miary jakości modelu IV `prSummary`

Podobnie jest w przypadku funkcji `prSummary` 

```{r}
pS <- prSummary(summary_cf, lev = levels(summary_cf$obs))
pS[1]
```